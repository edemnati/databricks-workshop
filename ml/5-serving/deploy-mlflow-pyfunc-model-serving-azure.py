# Databricks notebook source
# MAGIC %md
# MAGIC # Deploy an MLflow `pyfunc` model with Model Serving
# MAGIC
# MAGIC In this notebook, learn how to deploy MLflow `pyfunc` to a serving endpoint. MLflow pyfunc offers greater flexibility and customization to your deployment. You can run any custom model, add preprocessing or post-processing logic, or execute any arbitrary Python code. While using the MLflow built-in flavor is recommended for optimal performance, you can use MLflow `pyfunc` where more customization is required. The following demonstrates how to deploy a GPT-2 model with `pyfunc` to a GPU endpoint, but the workflow outlined below also works with various other types of models to either CPU or GPU endpoints.

# COMMAND ----------

# MAGIC %md
# MAGIC ## Install and import libraries 

# COMMAND ----------

!pip install --upgrade mlflow
!pip install --upgrade transformers
!pip install --upgrade accelerate
dbutils.library.restartPython()

# COMMAND ----------

import os
import pandas as pd
import requests
import json
from transformers import pipeline
import mlflow
from mlflow.models import infer_signature
from mlflow.transformers import generate_signature_output
from mlflow.tracking import MlflowClient

# COMMAND ----------

# MAGIC %md
# MAGIC ## Initialize and configure your model
# MAGIC
# MAGIC Download your model and package it with the model container.

# COMMAND ----------


model = pipeline('text-generation', model='gpt2', device_map = "auto")
snapshot_location = os.path.expanduser("~/.cache/huggingface/model")
os.makedirs(snapshot_location, exist_ok=True)
model.save_pretrained(snapshot_location)

# COMMAND ----------

class gpt2(mlflow.pyfunc.PythonModel):
    def __init__(self):
        """
        Initialize instance variables here if necessary.
        """
        self.generator = None

    def load_context(self, context):
        """
        Method to initialize the transformer model with the specified model from the artifact repository.
        """
        self.generator = pipeline('text-generation', model=context.artifacts['repository'], device_map='auto')

    def preprocess(self, model_input):
        """
        Method to preprocess the input data before passing it to the predict method.
        """
        # Implement any necessary preprocessing here
        return model_input

    def postprocess(self, model_output):
        """
        Method to postprocess the output data generated by the predict method.
        """
        # Implement any necessary postprocessing here
        return model_output

    def predict(self, context, model_input, params):
        """
        Method to generate predictions for the given input.
        """
        outputs = []
        processed_input = self.preprocess(model_input)

        for i, prompt in enumerate(processed_input.get("prompt", [])):
            outputs.append(self.generator(prompt, **params))
            
        outputs = self.postprocess(outputs)
        
        return outputs


# COMMAND ----------

# MAGIC %md
# MAGIC ## Log your model using MLflow
# MAGIC
# MAGIC The following code defines the inference parameters to pass to the model at the time of inference and the model schema.

# COMMAND ----------


inference_config = {"max_new_tokens": 100, "temperature": 1}

input_example = pd.DataFrame({"prompt": ["Hello, I'm a language model,"]})
inference_config={"max_new_tokens": 10, "temperature": 1}
output = generate_signature_output(model, input_example)
signature = infer_signature(input_example, output, params=inference_config)

# COMMAND ----------

# MAGIC %md
# MAGIC Log the model with the MLflow `pyfunc` flavor.

# COMMAND ----------


with mlflow.start_run():
    model_info = mlflow.pyfunc.log_model(
        "model",
        python_model=gpt2(),
        artifacts={'repository' : snapshot_location},
        pip_requirements=["torch", "transformers", "accelerate"],
        input_example = input_example,
        signature = signature,
        registered_model_name = "gpt2py",
    )

# COMMAND ----------

# MAGIC %md
# MAGIC ## Test your model in a notebook

# COMMAND ----------


# Load the model
my_sentence_generator = mlflow.pyfunc.load_model(model_info.model_uri)

# Generate a prediction using the loaded model with the given parameters
my_sentence_generator.predict(
    pd.DataFrame({"prompt": ["Hello, I'm a language model,"]}),
    params={"max_new_tokens": 5, "temperature": 1}
)

# COMMAND ----------

# MAGIC %md
# MAGIC ## Configure and create your model serving endpoint
# MAGIC
# MAGIC The following variables set the values for configuring the model serving endpoint, such as the endpoint name, compute type, and which model to serve with the endpoint. After you call the create endpoint API, the logged model is deployed to the endpoint.

# COMMAND ----------

# Set the name of the MLflow endpoint
endpoint_name = "gpt2py"

# Name of the registered MLflow model
model_name = "gpt2py" 

# Get the latest version of the MLflow model
model_version = MlflowClient().get_registered_model(model_name).latest_versions[0].version 

# Specify the type of compute (CPU, GPU_SMALL, GPU_LARGE, etc.)
workload_type = "GPU_LARGE" 

# Specify the scale-out size of compute (Small, Medium, Large, etc.)
workload_size = "Small" 

# Specify Scale to Zero(only supported for CPU endpoints)
scale_to_zero = False 

# Get the API endpoint and token for the current notebook context
API_ROOT = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiUrl().get() 
API_TOKEN = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().get()

# COMMAND ----------

# MAGIC %md
# MAGIC The follwoing sends the POST request to create the serving endpoint.

# COMMAND ----------



data = {
    "name": endpoint_name,
    "config": {
        "served_entities": [
            {
                "entity_name": model_name,
                "entity_version": model_version,
                "workload_size": workload_size,
                "scale_to_zero_enabled": scale_to_zero,
                "workload_type": workload_type,
            }
        ]
    },
}

headers = {"Context-Type": "text/json", "Authorization": f"Bearer {API_TOKEN}"}

response = requests.post(
    url=f"{API_ROOT}/api/2.0/serving-endpoints", json=data, headers=headers
)

print(json.dumps(response.json(), indent=4))

# COMMAND ----------

# MAGIC %md
# MAGIC ## View your endpoint
# MAGIC To see more information about your endpoint, go to the **Serving** UI and search for your endpoint name.

# COMMAND ----------

# MAGIC %md
# MAGIC ## Query your endpoint
# MAGIC
# MAGIC Once your endpoint is ready, you can query it by making an API request. Depending on the model size and complexity, it can take 30 minutes or more for the endpoint to get ready.  

# COMMAND ----------

data = {
  "inputs" : ["Hello, I'm a language model,"],
  "params" : {"max_new_tokens": 10, "temperature": 1}
}

headers = {"Context-Type": "text/json", "Authorization": f"Bearer {API_TOKEN}"}

response = requests.post(
    url=f"{API_ROOT}/serving-endpoints/{endpoint_name}/invocations", json=data, headers=headers
)

print(json.dumps(response.json()))
